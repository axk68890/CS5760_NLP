{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2.2 — Code a mini-BPE learner\n",
    "\n",
    "Toy Corpus: low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new\n",
    "\n"
   ],
   "id": "6742e408ea3cf3ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1.\tUse the classroom code above (or your own) to learn BPE merges for the toy corpus.\n",
    "o\tPrint the top pair at each step and the evolving vocabulary size.\n"
   ],
   "id": "d21adf236b21528e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T01:49:43.465463Z",
     "start_time": "2026-02-08T01:49:43.457336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# 1. Original corpus with frequencies\n",
    "corpus_tokens = \"\"\"\n",
    "low low low low low lowest lowest newer newer newer\n",
    "newer newer newer wider wider wider new new\n",
    "\"\"\".split()\n",
    "\n",
    "def make_vocab(tokens):\n",
    "    \"\"\"\n",
    "    Building initial vocab: chars + </w>(end of word) with frequencies\n",
    "    \"\"\"\n",
    "    freqs = Counter(tokens)\n",
    "    vocab = Counter()\n",
    "    for w, f in freqs.items():\n",
    "        # appending space ' '  between the characters and also end of work marker </w>\n",
    "        chars = \" \".join(list(w)) + \" </w>\"\n",
    "        vocab[chars] += f\n",
    "\n",
    "    print('vocab : ',vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = make_vocab(corpus_tokens)\n",
    "\n",
    "def get_frequencies(vocab):\n",
    "    \"\"\"\n",
    "    Counting adjacent character pair frequencies\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        # counting adj pair frequencies\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    \"\"\"\n",
    "    Merging pairs of characters in vocab\n",
    "    \"\"\"\n",
    "    a, b = pair\n",
    "    bigram = \" \".join(pair)\n",
    "    merged = a + b\n",
    "    new_vocab = Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(bigram, merged)\n",
    "        new_vocab[new_word] += freq\n",
    "    return new_vocab\n",
    "\n",
    "# 2. BPE(Byte-Pair Encoding) merging\n",
    "print(\"Initial vocab size:\", len(vocab))\n",
    "merges = []\n",
    "\n",
    "# no.of steps for merging. setting default to 15\n",
    "no_of_merge_steps = 15\n",
    "\n",
    "for i in range(no_of_merge_steps):\n",
    "    stats = get_frequencies(vocab)\n",
    "    if not stats:\n",
    "        break\n",
    "    # getting the most frequent adj pair based on the frequency count\n",
    "    best = max(stats, key=stats.get)\n",
    "    merges.append(best)\n",
    "    print('merges : ', merges)\n",
    "\n",
    "    print(f\"Step {i+1}: '{best[0]}+{best[1]}' (count={stats[best]})\")\n",
    "    print(f\"Vocab size: {len(vocab)} → {len(merge_vocab(best, vocab))}\")\n",
    "    vocab = merge_vocab(best, vocab)\n"
   ],
   "id": "a36a607d922440ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab :  Counter({'n e w e r </w>': 6, 'l o w </w>': 5, 'w i d e r </w>': 3, 'l o w e s t </w>': 2, 'n e w </w>': 2})\n",
      "Initial vocab size: 5\n",
      "merges :  [('e', 'r')]\n",
      "Step 1: 'e+r' (count=9)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>')]\n",
      "Step 2: 'er+</w>' (count=9)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e')]\n",
      "Step 3: 'n+e' (count=8)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e'), ('ne', 'w')]\n",
      "Step 4: 'ne+w' (count=8)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e'), ('ne', 'w'), ('l', 'o')]\n",
      "Step 5: 'l+o' (count=7)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e'), ('ne', 'w'), ('l', 'o'), ('lo', 'w')]\n",
      "Step 6: 'lo+w' (count=7)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e'), ('ne', 'w'), ('l', 'o'), ('lo', 'w'), ('new', 'er</w>')]\n",
      "Step 7: 'new+er</w>' (count=6)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e'), ('ne', 'w'), ('l', 'o'), ('lo', 'w'), ('new', 'er</w>'), ('low', '</w>')]\n",
      "Step 8: 'low+</w>' (count=5)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e'), ('ne', 'w'), ('l', 'o'), ('lo', 'w'), ('new', 'er</w>'), ('low', '</w>'), ('w', 'i')]\n",
      "Step 9: 'w+i' (count=3)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e'), ('ne', 'w'), ('l', 'o'), ('lo', 'w'), ('new', 'er</w>'), ('low', '</w>'), ('w', 'i'), ('wi', 'd')]\n",
      "Step 10: 'wi+d' (count=3)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e'), ('ne', 'w'), ('l', 'o'), ('lo', 'w'), ('new', 'er</w>'), ('low', '</w>'), ('w', 'i'), ('wi', 'd'), ('wid', 'er</w>')]\n",
      "Step 11: 'wid+er</w>' (count=3)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e'), ('ne', 'w'), ('l', 'o'), ('lo', 'w'), ('new', 'er</w>'), ('low', '</w>'), ('w', 'i'), ('wi', 'd'), ('wid', 'er</w>'), ('low', 'e')]\n",
      "Step 12: 'low+e' (count=2)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e'), ('ne', 'w'), ('l', 'o'), ('lo', 'w'), ('new', 'er</w>'), ('low', '</w>'), ('w', 'i'), ('wi', 'd'), ('wid', 'er</w>'), ('low', 'e'), ('lowe', 's')]\n",
      "Step 13: 'lowe+s' (count=2)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e'), ('ne', 'w'), ('l', 'o'), ('lo', 'w'), ('new', 'er</w>'), ('low', '</w>'), ('w', 'i'), ('wi', 'd'), ('wid', 'er</w>'), ('low', 'e'), ('lowe', 's'), ('lowes', 't')]\n",
      "Step 14: 'lowes+t' (count=2)\n",
      "Vocab size: 5 → 5\n",
      "merges :  [('e', 'r'), ('er', '</w>'), ('n', 'e'), ('ne', 'w'), ('l', 'o'), ('lo', 'w'), ('new', 'er</w>'), ('low', '</w>'), ('w', 'i'), ('wi', 'd'), ('wid', 'er</w>'), ('low', 'e'), ('lowe', 's'), ('lowes', 't'), ('lowest', '</w>')]\n",
      "Step 15: 'lowest+</w>' (count=2)\n",
      "Vocab size: 5 → 5\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2.\tSegment the words: new, newer, lowest, widest, and one word you invent (e.g., newestest).\n",
    "o\tInclude the subword sequence produced (tokens with _ where applicable).\n"
   ],
   "id": "c8e597341a05a3f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T16:03:06.397874Z",
     "start_time": "2026-02-02T16:03:06.391930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Segmenting test words (with _ convention)\n",
    "def get_tokens(word, merges):\n",
    "    \"\"\"\n",
    "    Applying merges to get subword tokens\n",
    "    \"\"\"\n",
    "    symbols = list(word) + [\"</w>\"]\n",
    "    print(f'word : {word} --> symbols : {symbols}')\n",
    "    for a, b in merges:\n",
    "        i = 0\n",
    "        # check if the merge characters are present in the current word symbols and merge\n",
    "        while i < len(symbols)-1:\n",
    "            if symbols[i] == a and symbols[i+1] == b:\n",
    "                symbols[i] = a + b\n",
    "                del symbols[i+1]\n",
    "            else:\n",
    "                i += 1\n",
    "    if symbols[-1] == \"</w>\":\n",
    "        symbols.pop()\n",
    "    # Convert to subword format with _\n",
    "    tokens = []\n",
    "    for s in symbols:\n",
    "        if s == ' ':\n",
    "            tokens[-1] += '_'\n",
    "        else:\n",
    "            tokens.append(s.replace(' ', '_'))\n",
    "    return tokens\n",
    "\n",
    "test_words = [\"new\", \"newer\", \"lowest\", \"widest\", \"newestest\"]\n",
    "print(\"\\nSegmentations:\")\n",
    "for w in test_words:\n",
    "    tokens = get_tokens(w, merges)\n",
    "    print(f\"{w} → {' '.join(tokens)}\")"
   ],
   "id": "e5a641f279a2fbc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmentations:\n",
      "word : new --> symbols : ['n', 'e', 'w', '</w>']\n",
      "new → new\n",
      "word : newer --> symbols : ['n', 'e', 'w', 'e', 'r', '</w>']\n",
      "newer → newer</w>\n",
      "word : lowest --> symbols : ['l', 'o', 'w', 'e', 's', 't', '</w>']\n",
      "lowest → lowest</w>\n",
      "word : widest --> symbols : ['w', 'i', 'd', 'e', 's', 't', '</w>']\n",
      "widest → wid e s t\n",
      "word : newestest --> symbols : ['n', 'e', 'w', 'e', 's', 't', 'e', 's', 't', '</w>']\n",
      "newestest → new e s t e s t\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3.\tIn 5–6 sentences, explain:\n",
    "o\tHow subword tokens solved the OOV (out-of-vocabulary) problem.\n",
    "o\tOne example where subwords align with a meaningful morpheme (e.g., er_ as English agent/comparative suffix)."
   ],
   "id": "dce7f9af0a47ab84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "BPE Explanation\n",
    "Subword tokenization solves the out-of-vocabulary (OOV) problem by breaking unknown words into smaller pieces drawn from a fixed vocabulary of character sequences. Instead of mapping rare words like \"newestest\" to an <UNK> token, BPE composes it from learned subwords (new, est, est) that already exist in the vocabulary. This makes the token sequence interpretable and allows models to generalize across morphological patterns.\n",
    "\n",
    "One meaningful alignment occurs with er_ and est_ suffixes, which emerge naturally from the corpus frequency. In \"newer→new er_\" and \"lowest→low est_\", these subwords correspond to English comparative/superlative morphemes, letting the model capture productive grammatical patterns without explicit linguistic rules."
   ],
   "id": "98e3a9fae7555af0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2.3 — Your language (or English if you prefer)\n",
    "Pick one short paragraph (4–6 sentences) in your own language (or English if that’s simpler).\n",
    "\n",
    "\n"
   ],
   "id": "b4be68df9cda8a09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1.\tTrain BPE on that paragraph (or a small file of your choice).\n",
    "o\tUse end-of-word _.\n",
    "o\tLearn at least 30 merges (adjust if the text is very small)."
   ],
   "id": "81e4a84b11abf7b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T17:54:15.504865Z",
     "start_time": "2026-02-08T17:54:15.489860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "# Training paragraph (English)\n",
    "paragraph = \"\"\"\n",
    "At 186 miles per hour, the landscape starts to blur. A mile disappears every 20 seconds. An entire town can blink by in the time it takes to remember its name. High-speed trains are, as the name suggests, fast. When two are traveling toward each other, the distance between them can close at double their top speed. Sixty seconds after they pass — a moment when the drivers barely have the chance to exchange a friendly wave — they can be six miles apart, relentlessly powering across the countryside, each with more than 1,000 passengers on board. That breathtaking pace makes the recent deadly crash between two high-speed trains in Spain even more alarming. Despite an extraordinary global safety record, the derailment on a straight stretch of track was a stark reminder of how exceptional, and how fragile, these systems can be.\n",
    "\"\"\"\n",
    "\n",
    "# Using end-of-word _ instead of </w>\n",
    "def preprocess(corpus):\n",
    "    \"\"\"Splitting into words, adding _ end marker\"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', corpus.lower())\n",
    "    vocab = Counter()\n",
    "    for w in words:\n",
    "        chars = \" \".join(list(w)) + \" _\"\n",
    "        vocab[chars] += 1\n",
    "    return vocab\n",
    "\n",
    "vocab = preprocess(paragraph)\n",
    "print(f\"Initial vocab size: {len(vocab)}\")\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    a, b = pair\n",
    "    bigram = f\"{a} {b}\"\n",
    "    merged = a + b\n",
    "    new_vocab = Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(bigram, merged)\n",
    "        new_vocab[new_word] += freq\n",
    "    return new_vocab\n",
    "\n",
    "# Training 30+ merges\n",
    "merges = []\n",
    "merge_count = 35\n",
    "for i in range(merge_count):\n",
    "    stats = get_stats(vocab)\n",
    "    if not stats:\n",
    "        break\n",
    "    best = max(stats, key=stats.get)\n",
    "    merges.append(best)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "\n",
    "print(f\"\\nLearned {len(merges)} merges\")"
   ],
   "id": "4e0ae3342c73f02d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab size: 105\n",
      "\n",
      "Learned 35 merges\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.\tShow the five most frequent merges and the resulting five longest subword tokens.\n",
   "id": "23ec3213c124819b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T17:54:19.221387Z",
     "start_time": "2026-02-08T17:54:19.217345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Showing 5 most frequent merges & 5 longest subword tokens\n",
    "print(\"\\nTop 5 most frequent merges:\")\n",
    "for i, (a,b) in enumerate(merges[:5], 1):\n",
    "    print(f\"{i}: '{a}+{b}'\")\n",
    "\n",
    "print(\"\\n5 longest subword tokens in final vocab:\")\n",
    "longest = sorted(vocab, key=len, reverse=True)[:5]\n",
    "for token in longest:\n",
    "    print(f\"  {repr(token)} (freq: {vocab[token]})\")"
   ],
   "id": "ee4a652f89d02ebc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 most frequent merges:\n",
      "1: 'e+_'\n",
      "2: 's+_'\n",
      "3: 't+h'\n",
      "4: 'n+_'\n",
      "5: 'a+r'\n",
      "\n",
      "5 longest subword tokens in final vocab:\n",
      "  'e x c e p t i o n a l _' (freq: 1)\n",
      "  'e x tra o r d in ar y_' (freq: 1)\n",
      "  're l en t l e s s ly_' (freq: 1)\n",
      "  'co u n tr y s i d e_' (freq: 1)\n",
      "  'b re a th t a k ing_' (freq: 1)\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3.\tSegment 5 different words from the paragraph:\n",
    "o\tInclude one rare word and one derived/inflected form.\n"
   ],
   "id": "c33ecd28277ec7f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T17:54:31.562978Z",
     "start_time": "2026-02-08T17:54:31.557470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Segmentation\n",
    "def segment(word, merges):\n",
    "    symbols = list(word.lower()) + [\"_\"]\n",
    "    for a, b in merges:\n",
    "        i = 0\n",
    "        while i < len(symbols)-1:\n",
    "            if symbols[i] == a and symbols[i+1] == b:\n",
    "                symbols[i] = a + b\n",
    "                del symbols[i+1]\n",
    "            else:\n",
    "                i += 1\n",
    "    if symbols[-1] == \"_\":\n",
    "        symbols.pop()\n",
    "    return symbols\n",
    "\n",
    "# 5 words - including one rare and one derived/inflected form\n",
    "test_words = [\"landscape\", \"stark\", \"blur\", \"alarming\", \"1,000\"]\n",
    "print(\"\\nSegmentations:\")\n",
    "for w in test_words:\n",
    "    tokens = segment(w, merges)\n",
    "    print(f\"{w} → {' '.join(tokens)}\")"
   ],
   "id": "b289c0378c25178c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmentations:\n",
      "landscape → l an d s c a p e_\n",
      "stark → st ar k\n",
      "blur → b l u r\n",
      "alarming → a l ar m ing_\n",
      "1,000 → 1 , 0 0 0\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4.\tBrief reflection (5–8 sentences):\n",
    "o\tWhat kinds of subwords were learned (prefixes, suffixes, stems, whole words)?\n",
    "o\tTwo concrete pros/cons of subword tokenization for your language."
   ],
   "id": "a236534a98b29b08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Reflection:\n",
    "\n",
    "With 35 merges, BPE learned mostly s+_, e+_ suffixes (morphology) and t+h, a+r bigrams (inside-word patterns). No prefixes or stems emerged due to small corpus size. Longest \"subwords\" were actually whole rare words that didn't get merged.\n",
    "\n",
    "PRO 1: Shared suffixes (s+_) mean \"trains_\" and \"starts_\" use same ending leading to morphological generalization.\n",
    "PRO 2: Handles inflection automatically. Meaning any new plural gets s+_.\n",
    "CON 1: Rare words like \"exceptional_\" stay character-level (11 tokens) - verbose encoding.\n",
    "CON 2: Small corpus limited us to 5 useful merges; needs larger text corpus for real morphology.\n",
    "\n"
   ],
   "id": "3e3abc1121dbd00a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ec024523bca3a97b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Q5. Programming Question:",
   "id": "5705934a3184bfe9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. Compare with a Tool\n",
    "Run the paragraph through an NLP tool that supports your language (e.g., NLTK, spaCy, or any open-source tokenizer if available).\n",
    "•\tCompare tool output vs. your manual tokens.\n",
    "•\tWhich tokens differ? Why?\n"
   ],
   "id": "f447eae08090d28a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Paragraph : A stroll along Mohegan Bluffs on Block Island is one of the most dramatic walks available in the Ocean State. The bluffs stand 200 feet above the beach and, on a clear day, provide views straight across the Block Island Sound to Montauk, New York. After a stroll, check out the picturesque lighthouses that dot Block Island.",
   "id": "74719bbdaaa8f7f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T17:48:45.453676Z",
     "start_time": "2026-02-08T17:48:45.445907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "paragraph = \"\"\"A stroll along Mohegan Bluffs on Block Island is one of the most dramatic walks available in the Ocean State. The bluffs stand 200 feet above the beach and, on a clear day, provide views straight across the Block Island Sound to Montauk, New York. After a stroll, check out the picturesque lighthouses that dot Block Island.\"\"\"\n",
    "\n",
    "def manual_tokenize(text):\n",
    "    tokens = text.split()\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token.endswith('.'):\n",
    "            result.extend([token[:-1], '.'])\n",
    "        elif token.endswith(','):\n",
    "            result.extend([token[:-1], ','])\n",
    "        else:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "naive_tokens = paragraph.split()\n",
    "manual_tokens = manual_tokenize(paragraph)\n",
    "nltk_tokens = nltk.word_tokenize(paragraph)\n",
    "\n",
    "print(f'\\nNaive Tokens : {naive_tokens}')\n",
    "print(f'\\nManual Tokens : {manual_tokens}')\n",
    "print(f'\\nNLTK Tokens : {nltk_tokens}')\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(\"TOKEN COMPARISON :\")\n",
    "\n",
    "# comparing naive and manual tokens using prefix matching\n",
    "\n",
    "\n",
    "manual_idx = 0 # tracking current manual token position\n",
    "\n",
    "print(\"\\nComparison Results (Naive vs Manual):\")\n",
    "print(f\"{'Naive Pos':<8} {'Old Token':<12} {'Manual Pos':<10} {'New Tokens':<20} {'Match?'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for naive_idx, old_token in enumerate(naive_tokens):\n",
    "    if old_token.endswith(('.', ',')):\n",
    "        # Check if manual tokens match the prefix\n",
    "        expected_prefix = old_token[:-1]\n",
    "\n",
    "        # Look ahead in manual_tokens for prefix match\n",
    "        if (manual_idx < len(manual_tokens) and\n",
    "            manual_tokens[manual_idx] == expected_prefix and\n",
    "            manual_idx + 1 < len(manual_tokens) and\n",
    "            manual_tokens[manual_idx + 1] == old_token[-1]):\n",
    "\n",
    "            print(f\"{naive_idx:<8} '{old_token}'{'':<4} {manual_idx}:{manual_idx+1}{'':<4} \"\n",
    "                  f\"'{manual_tokens[manual_idx]}' + '{manual_tokens[manual_idx+1]}'      YES\")\n",
    "            manual_idx += 2  # Skip both word and punctuation\n",
    "        else:\n",
    "            print(f\"{naive_idx:<8} '{old_token}'{'':<4} {manual_idx}{'':<10} \"\n",
    "                  f\"MISMATCH! Expected '{expected_prefix}'      NO\")\n",
    "    else:\n",
    "        # Non-punctuation token - should match exactly\n",
    "        if manual_idx < len(manual_tokens) and manual_tokens[manual_idx] == old_token:\n",
    "            manual_idx += 1\n",
    "        else:\n",
    "            print(f\"{naive_idx:<8} '{old_token}' → MISMATCH at manual[{manual_idx}]     \")\n",
    "\n",
    "\n",
    "print(f\"\\nSummary: Processed {len(naive_tokens)} naïve -> {len(manual_tokens)} manual tokens\")\n",
    "print(f\"Manual pointer ended at: {manual_idx} (should be {len(manual_tokens)})\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(\"\\nComparison Results (Manual vs NLTK):\")\n",
    "\n",
    "# comparing manual and nltk tokens\n",
    "if(len(manual_tokens) == len(nltk_tokens)):\n",
    "    mismatch_count = 0\n",
    "    for manual_idx, manual_token in enumerate(manual_tokens):\n",
    "        if manual_token != nltk_tokens[manual_idx]:\n",
    "            print(f'Manual Token : {manual_token} does not match with NLTK Token : {nltk_tokens[manual_idx]} at Index : {manual_idx}')\n",
    "            mismatch_count += 1\n",
    "\n",
    "    if mismatch_count == 0:\n",
    "        print(f'All {len(manual_tokens)} Manual Tokens match NLTK Tokens')\n",
    "    else:\n",
    "        print(f' {mismatch_count} Manual Tokens did not match NLTK Tokens')\n",
    "\n",
    "else:\n",
    "    print(\"Length of manual tokens does not match length of NLTK tokens\")\n",
    "\n"
   ],
   "id": "83efa336ae1ab92b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Tokens : ['A', 'stroll', 'along', 'Mohegan', 'Bluffs', 'on', 'Block', 'Island', 'is', 'one', 'of', 'the', 'most', 'dramatic', 'walks', 'available', 'in', 'the', 'Ocean', 'State.', 'The', 'bluffs', 'stand', '200', 'feet', 'above', 'the', 'beach', 'and,', 'on', 'a', 'clear', 'day,', 'provide', 'views', 'straight', 'across', 'the', 'Block', 'Island', 'Sound', 'to', 'Montauk,', 'New', 'York.', 'After', 'a', 'stroll,', 'check', 'out', 'the', 'picturesque', 'lighthouses', 'that', 'dot', 'Block', 'Island.']\n",
      "\n",
      "Manual Tokens : ['A', 'stroll', 'along', 'Mohegan', 'Bluffs', 'on', 'Block', 'Island', 'is', 'one', 'of', 'the', 'most', 'dramatic', 'walks', 'available', 'in', 'the', 'Ocean', 'State', '.', 'The', 'bluffs', 'stand', '200', 'feet', 'above', 'the', 'beach', 'and', ',', 'on', 'a', 'clear', 'day', ',', 'provide', 'views', 'straight', 'across', 'the', 'Block', 'Island', 'Sound', 'to', 'Montauk', ',', 'New', 'York', '.', 'After', 'a', 'stroll', ',', 'check', 'out', 'the', 'picturesque', 'lighthouses', 'that', 'dot', 'Block', 'Island', '.']\n",
      "\n",
      "NLTK Tokens : ['A', 'stroll', 'along', 'Mohegan', 'Bluffs', 'on', 'Block', 'Island', 'is', 'one', 'of', 'the', 'most', 'dramatic', 'walks', 'available', 'in', 'the', 'Ocean', 'State', '.', 'The', 'bluffs', 'stand', '200', 'feet', 'above', 'the', 'beach', 'and', ',', 'on', 'a', 'clear', 'day', ',', 'provide', 'views', 'straight', 'across', 'the', 'Block', 'Island', 'Sound', 'to', 'Montauk', ',', 'New', 'York', '.', 'After', 'a', 'stroll', ',', 'check', 'out', 'the', 'picturesque', 'lighthouses', 'that', 'dot', 'Block', 'Island', '.']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TOKEN COMPARISON :\n",
      "\n",
      "Comparison Results (Naive vs Manual):\n",
      "Naive Pos Old Token    Manual Pos New Tokens           Match?\n",
      "----------------------------------------------------------------------\n",
      "19       'State.'     19:20     'State' + '.'      YES\n",
      "28       'and,'     29:30     'and' + ','      YES\n",
      "32       'day,'     34:35     'day' + ','      YES\n",
      "42       'Montauk,'     45:46     'Montauk' + ','      YES\n",
      "44       'York.'     48:49     'York' + '.'      YES\n",
      "47       'stroll,'     52:53     'stroll' + ','      YES\n",
      "56       'Island.'     62:63     'Island' + '.'      YES\n",
      "\n",
      "Summary: Processed 57 naïve -> 64 manual tokens\n",
      "Manual pointer ended at: 64 (should be 64)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Comparison Results (Manual vs NLTK):\n",
      "All 64 Manual Tokens match NLTK Tokens\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. Compare with a Tool\n",
    "Run the paragraph through an NLP tool that supports your language (e.g., NLTK, spaCy, or any open-source tokenizer if available).\n",
    "•\tCompare tool output vs. your manual tokens.\n",
    "•\tWhich tokens differ? Why?\n"
   ],
   "id": "f0d9beb48e1b771e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tokenized the paragraph using NLTK. Below is the summary:\n",
    "• Naïve:     57 tokens\n",
    "• Manual:    64 tokens\n",
    "• NLTK:      64 tokens\n",
    "\n",
    "Comparison Results(Manual vs NLTK):\n",
    "\n",
    "All 64 Manual tokens match NLTK tokens\n",
    "\n",
    "Comparison Results(Naive vs Manual):\n",
    "\n",
    "Naive Pos Old Token    Manual Pos New Tokens           Match?\n",
    "----------------------------------------------------------------------\n",
    "19       'State.'     19:20     'State' + '.'      YES\n",
    "28       'and,'     29:30     'and' + ','      YES\n",
    "32       'day,'     34:35     'day' + ','      YES\n",
    "42       'Montauk,'     45:46     'Montauk' + ','      YES\n",
    "44       'York.'     48:49     'York' + '.'      YES\n",
    "47       'stroll,'     52:53     'stroll' + ','      YES\n",
    "56       'Island.'     62:63     'Island' + '.'      YES\n",
    "\n",
    "Summary: Processed 57 naïve → 64 manual tokens\n",
    "Manual pointer ended at: 64 (should be 64)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "df98dcdbb406b176"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "876080eccb7a6ea6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. Multiword Expressions (MWEs)\n",
    "Identify at least 3 multiword expressions (MWEs) in your language. Example:\n",
    "•\tPlace names, idioms, or common fixed phrases.\n",
    "•\tExplain why they should be treated as single tokens.\n"
   ],
   "id": "8ac9d202d470a145"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Mohegan Bluffs - refers to a particular landmark on Block Island, not generic “Mohegan” + “bluffs.” As a single token, this represents one landmark entity\n",
    "2. Block Island - This is a specific geographic location, not just any “block” and any “island.”\n",
    "3. Ocean State - This is official nickname of Rhode Island\n",
    "4. Block Island Sound - Block Island Sound is a roughly 10mile wide, 40meter deep marine sound in the Atlantic Ocean separating Block Island from mainland Rhode Island.\n",
    "\n",
    "The above MWEs should be treated as single tokens because, their meaning is not just the sum of individual words. They correspond to a single named entity/ land mark / location"
   ],
   "id": "8f2e7fed29113f7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. Reflection (5–6 sentences)",
   "id": "3742ad870290ca51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Yes, punctuation, morphology, and MWEs significantly complicate tokenization.\n",
    "\n",
    "1. Punctuation attaches to words without spaces. Tracking index shifts when splitting punctuation from words during the manual tokenization was time consuming however the prefix-matching comparison perfectly validates manual tokenization.\n",
    "2. English tokenizers rarely split morphemes, but stemming/lemmatization needs morphology\n",
    "3. MWE need external knowledge/lexicon to tokenize them\n",
    "\n",
    "Libraries like NLTK, spaCy automatically solve most of these issues and make it look easy but manually tokenizing the paragraph revealed the level of difficulty and the external knowledge/lexicon that is needed to tokenize.\n",
    "\n"
   ],
   "id": "379c17f4c46542fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
